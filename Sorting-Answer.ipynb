{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2cb634a-9674-4010-b694-421cd4bddb4a",
   "metadata": {},
   "source": [
    "# Sorting-Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bff9e-027a-4245-9870-7d81e22f6592",
   "metadata": {},
   "source": [
    "#### By Dae Hyeun (Isaac) Cheong "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c3dfa-ce42-41b2-9387-c6119e269acd",
   "metadata": {},
   "source": [
    "This document has a code to sort a free response question into 4 categories: \n",
    "* **MC**: Most likely to be a correct answer\n",
    "* **CC**: Likely to be a correct answer but need more careful look \n",
    "* **MI**: Most likely to be a incorrect answer but need more careful look\n",
    "* **I**: Incorrect answer that need 'no check' (Most likely to be a blank answer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecf6dac-f483-4e5f-a916-583e6ab934f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:2:13: unexpected '['\n1: {\n2:     \"tags\": [\n               ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:2:13: unexpected '['\n1: {\n2:     \"tags\": [\n               ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide-cell\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "library(knitr)\n",
    "library(dplyr)\n",
    "library(readxl)\n",
    "Items_28_and_30_2 <- read_excel(\"Items 28 and 30-2.xls\")\n",
    "translated_pc <- read_excel(\"translated_pc.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6845ae-9689-4e44-bc26-54745deb7bf4",
   "metadata": {},
   "source": [
    "#### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60370cc3-daac-429d-b3ae-0d4e21c8d4f3",
   "metadata": {},
   "source": [
    "To explain the methodology on how to sort an answer, I will use the question about water bubbles, which coresspond to question 4 of the science assessment. To the original dataset, the answer to the question 4 correspond to the column name \"pc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a686f42-a095-474b-94ec-d64e84fd5255",
   "metadata": {},
   "source": [
    "![AnswerRuburic](scoringrubric.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47a1bdd-f321-48df-9cf6-9feea9ed4a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e34310-22c6-4b77-8c66-cf1ff10466cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
